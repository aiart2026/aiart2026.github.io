<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>AIART2026</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <!-- <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
  <link
    href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
    rel='stylesheet' type='text/css'>
  <link
    href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic'
    rel='stylesheet' type='text/css'>

  <!-- Plugin CSS -->
  <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/creative.min.css" rel="stylesheet">
  <link rel="icon" type="images/png" href="images/logo1.png">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container"style="margin-left: 150px;">
      <a class="navbar-brand js-scroll-trigger" href="#page-top"><img src="images/logo.png" style="height: 50px;" /></a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">

        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-center" id="navbarResponsive">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#cfp">CFP</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#dates">Dates</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#keynotes">Keynotes</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#program">Program</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#ps">TPC</a>
             </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#ppl">Organizers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#MIR">Partners</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#sponsorship">Sponsors</a>
              </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#Gallery">Gallery</a>
              </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#Book">Book</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#history">History</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <header class="masthead text-center text-white d-flex">
    <div class="container my-auto">
      <div class="row">
        <div class="col-lg-10 mx-auto">
          <!-- <h2 class="text-uppercase"> -->
          <h2 style="font-size: 36px">
            <strong>The 8<sup>th</sup> IEEE Workshop on <br />
             Artificial Intelligence for
                Art Creation</strong>
          </h2>
          <hr>
        </div>
        <div class="col-lg-8 mx-auto">
          <p class="text-faded mb-5" style="font-size: 24px; font-weight: bold;">Bangkok, Thailand<br />July 5 - July 9, 2026<br />
            Jointly with <a href="https://2026.ieeeicme.org/" target="_blank" style="color: white"><u>ICME
                2026</u></a>
            <br>
          </p>
        </div>
      </div>
    </div>
  </header>

  <section class="bg-primary" id="cfp">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Call for Papers</h2>
          <hr class="light my-4">
          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            Recent advances brought by Multimodal Large Language Model (MLLM), Multimodal Agents, and Embodied Intelligence have been powerful driving forces for art generation and understanding, drawing more and more attention from both academia and industry. Across creative fields, AI has already sparked new genres and experimentations in painting, music, film, storytelling, fashion and design. Researchers explore the human and AI co-creativity as well as the ethical implications of AI arts. AI has been applied to art historical research, cultural heritage revitalization, and media studies. The aesthetic value of AI generated content and AI’s impact on art appreciation have also been a contended subject in recent scholarship. AI has not only exhibited creative potential, but also stimulated research from diverse perspectives of neuroscience, cognitive science, psychology, literature, art history, media and communication studies. Despite all these promising features of AI for Art, we still have to face the many challenges such as the biases in AI models, lack of transparency and interpretability in algorithms, and copyright issues of training data and AI Art works.
          </p>

          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            This is the 8<sup>th</sup> AIART workshop to be held in conjunction with ICME 2026 in Bangkok, Thailand, and it aims to bring forward cutting-edge technologies and most recent advances in the area of AI art as well as perspectives from related disciplines.
          </p>

          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            The theme topic of AIART 2026 will be <strong>Multimodal Agents for AI Art</strong>. We plan to invite 5 keynote speakers to present their insightful perspectives on AI art.
          </p>
          <p class="text-faded mb-4" style="text-align: justify; color: white; font-size: 18px">
            We sincerely invite high-quality papers presenting or addressing issues related to AI art, including but not limited to the following topics:
          </p>
        <div style="color:white; text-align:left; font-size:18px; line-height:1.8;">

  <div style="font-weight:bold; margin-top:24px; margin-bottom:8px;">
    Track 1: Theories for AI Art
  </div>
  <ul>
    <li>Neuroscience</li>
    <li>Cognitive science and Psychology</li>
    <li>Aesthetics</li>
    <li>Creativity</li>
    <li>Arts (Fine Arts, Arts and Crafts, Performing Arts, Interdisciplinary Arts, Literature and Art)</li>
  </ul>

  <div style="font-weight:bold; margin-top:24px; margin-bottom:8px;">
    Track 2: AI for Art Generation
  </div>
  <ul>
    <li>AI for painting and calligraphy</li>
    <li>AI for video and movie</li>
    <li>AI for music and audio</li>
    <li>AI for literature</li>
    <li>AI for design</li>
    <li>AI for videogame</li>
    <li>Adaptive expression</li>
  </ul>

  <div style="font-weight:bold; margin-top:24px; margin-bottom:8px;">
    Track 3: AI for Art Understanding
  </div>
  <ul>
    <li>Affective computing</li>
    <li>Aesthetic evaluation</li>
    <li>Multimodal agents</li>
    <li>Embodied intelligence</li>
    <li>World foundation models</li>
  </ul>

  <div style="font-weight:bold; margin-top:24px; margin-bottom:8px;">
    Track 4: AI Art in Extended Reality (XR)
  </div>
  <ul>
    <li>AI-driven procedural generation for VR/AR worlds</li>
    <li>Virtual humans and digital performers</li>
    <li>AI choreography for volumetric video and motion capture</li>
    <li>Physics-aware and interactable generative assets</li>
  </ul>

  <div style="font-weight:bold; margin-top:24px; margin-bottom:8px;">
    Track 5: Human–AI Co-Creation &amp; Interaction
  </div>
  <ul>
    <li>Interactive AI tools for artists</li>
    <li>Real-time co-creation systems</li>
    <li>XR/VR/AR environments for human–AI creative expression</li>
    <li>Human–AI agency and authorship models</li>
    <li>Perception and UX research in creative tool design</li>
  </ul>

  <div style="font-weight:bold; margin-top:24px; margin-bottom:8px;">
    Track 6: AI for Humanity and the Humanities
  </div>
  <ul>
    <li>AI for cultural heritage</li>
    <li>AI for media studies</li>
    <li>AI for social justice</li>
    <li>AI for accessibility</li>
    <li>AI for empathy</li>
    <li>AI for textual analysis</li>
    <li>AI ethics and safety</li>
    <li>Authentication and IPR issues of AI artworks</li>
    <li>Deepfake detection for creative industries</li>
  </ul>

</div>
          <br>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
The authors of selected high-quality papers will be invited to submit an extended version to the Machine Intelligence Research (MIR) journal published by Springer, and the Transactions on Artificial Intelligence (TAI) journal published by Scilight.
          </p>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            Additionally, Best Paper Award will be given.
          </p>

          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            AIART 2026 will continue to organize the 2nd AIART Gallery for artists to showcase their creative AI artworks in the form of in-person gallery. The AIART Gallery will provide a great opportunity for people to experience interactive artworks and communicate creative ideas. 
          </p>
          <br>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 24px">
            <strong>Paper Submission</strong>
          </p>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            Authors should prepare their manuscript according to the Guide for Authors of ICME available at Author
            Information and Submission Instructions: <a href="https://2026.ieeeicme.org/author-information-and-submission-instructions/"
              target="_blank">https://2026.ieeeicme.org/author-information-and-submission-instructions/</a>
          </p>
          <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px">
            Submission address: <a href="https://cmt3.research.microsoft.com/ICMEW2026"
              target="_blank">https://cmt3.research.microsoft.com/ICMEW2026</a>
          </p>

          <!-- <p class="text-faded mb-4; " style="text-align: left; color: white; font-size: 18px" >
              Submission address: <a href="http://2023.ieeeicme.org/author-info.html"  target="_blank" >http://2023.ieeeicme.org/author-info.html</a>
            </p> -->

          <br>
          <a class="btn btn-light btn-xl js-scroll-trigger" href="https://cmt3.research.microsoft.com/ICMEW2026"
            target="_blank">Submit link</a>
        </div>
      </div>
    </div>
  </section>


  <section id="dates">
    <div class="container">
      <div class="row">
        <div class="col-lg-6 mx-auto text-center">
          <h2 class="section-heading">Important Dates</h2>
          <hr class="dark my-4">
          <table width="100%" cellpadding="10" align="center">
            <tr>
              <td>
                <div class="text-muted" style="text-align: left; font-weight: bold;">Submissions due</div>
              </td>
              <td>
                <div class="text-muted" style="text-align: left;">March 25, 2026</div>
              </td>
            </tr>
            <tr>
              <td>
                <div class="text-muted" style="text-align: left; font-weight: bold;">Workshop date</div>
              </td>
              <td>
                <div class="text-muted" style="text-align: left;">TBD</div>
              </td>
            </tr>


          </table>
          <!-- <a class="btn btn-light btn-xl js-scroll-trigger" href="#services">Get Started!</a> -->
        </div>
      </div>
    </div>
  </section>






  <!-- first -->
  <section  class="bg-primary" id="keynotes">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Keynotes (1/5)</h2>
          <hr class="light my-4">
  
          <p class="text-faded" style="text-align: left; font-weight: bold; color: white; font-size: 26px;">
            Keynote 1
          </p>
          <br>
          <!--   <p class="text-faded" style="text-align: left; font-weight: bold; color: white; font-size: 24px;">
            Speaker:TBD
          </p>
            -->
         <div style="float:right; clear:both; top:30px;">
            <img src="images/yongxiang.jpg" style="height: 250px; border-radius: 10px;" />
          </div>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>
  
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
           Yong Xiang
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>
  
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
           Aesthetics Intelligence-based Decoding, Activation and Utilization of Human Memeplex: from China's Practice
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
        TBD
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
          This keynote speech centers on the core concept of "aesthetic intelligence", exploring how to decode, activate and utilize human memes(cultural genes), with a focus on sharing China's innovative practices in relevant fields. The "Luoshen Project", hosted by the Institute of Cultural Industries at Peking University, aims to build core infrastructure supporting the digital development of Chinese culture. Its key achievement, the "Chinese Aestheme Project", systematically sorts out and digitally analyzes core traits of Chinese aesthetics such as "the interdependence of the virtual and the real" and "vital spirit and rhythmic vitality", classifying them into nine major meme categories.
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
          The "Luoshen Fu AI Creator", developed based on this project embodies the deep integration of culture and technology. It has evolved from a text-to-text tool in Version 1.0 into a multimodal intelligent agent with text-to-image and text-to-video capabilities. By training on a specialized dataset of traditional Chinese paintings and calligraphy works and applying advanced fine-tuning techniques, this tool strives to reproduce the unique "spiritual charm" and "artistic conception" of traditional Chinese art in digital generation, rather than merely imitating forms.
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
          This series of practices explores a new paradigm for the activation of cultural heritage in the AI era. It not only serves academic research and the cultural industry by lowering the threshold for creating "China-chic" content, but also has a deeper goal: to infuse intelligent agents with Chinese aesthetic wisdom, inherit and promote the spirit of Eastern aesthetics in the digital civilization era, and offer a reference for Thailand and other countries in implementing cultural digital and intelligent engineering.
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
           Dr. Yong Xiang is a Professor of the School of Arts at Peking University (SAPKU) and Dean of the Institute for Cultural Industries at Peking University (ICIPKU), where he is leader of Cultural Economics and Creative Management Project. His academic interest is focusing on cultural & creative industries, management of the art, culture and creativity. He has visited some universities in USA, UK, Italy, Australia, Japan, South Korea in these years. He was Academic Visitor, Judge Business School, the University of Cambridge (2010-2011),and visiting scholar at Faculty of Economics Marco Biagi, University of Modena, Reggio Emilia(Otc.2010), Italy. He was Honorary Visiting Professor of Business School at London Metropolitan University from 2010 to 2013, Guest Professor to The Chinese University of Hong Kong in 2019.He is a senior cultural and creative industries (CCIs) consultant to the governments, companies and institutions in China and UNCTAD. He was awarded one of the Top 10 Excellent Scholars of CCI in China in 2012. He is co-editors of China Cultural and Creative Industries Reports 2013, which is published by Springer and is a joint founder of the International Association of Culture and Creative Industries (London, HongKong), Member of the Advisory Committee of the International Center for Creativity and Sustainable Development under the auspices of UNESCO Since 2019. He is UNESCO Chairholder on Creativity and Sustainable Development in Rural Areas Since 2024.
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- second -->
    <section   id="keynotes-2">
      <div class="container">
        <div class="row">
          <div class="col-lg-10 mx-auto text-center">
            <h2 class="section-heading>Keynotes (2/5)</h2>
            <hr class="dark my-4">
    
            <p class="text-muted" style="text-align: left;font-weight: bold; font-size: 26px;">
              Keynote 2
            </p>
            <br>
           <!--        <p class="text-muted" style="text-align: left;font-weight: bold; font-size: 24px;">
            Speaker:TBD
          </p>
              -->
            <div style="float:right; clear:both; margin-left: 20px; margin-bottom: 20px;">
  <img src="images/kangzhang.png" style="width: 180px; height: auto; border-radius: 10px;" />
</div>
            <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
              Speaker:
            </p>
    
            <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
              Kang Zhang
            </p>
            <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
              Title:
            </p>
    
            <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
             Generative Art and Its Fusion of Eastern and Western Styles
            </p>
            <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
              Time:
            </p>
            <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
               TBD
            </p>
            <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
              Abstract:
            </p>
            <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
             In this talk, I will first focus on conceptual and theoretical foundation of generative art, part of computational aesthetics. Specifically, generative visual art is defined in terms of visual languages, and thus the roles of artists and generative systems become clear. I will then present our recent practices in generative art and design for the effective fusion of Eastern and Western artistic styles. The first practice combines the styles of Wassily Kandinsky and Wu Guanzhong, who are both great masters of using points, lines and planes in their paintings. The AI-fused Eastern and Western styles yield highly aesthetic images. The second practice applies AI techniques to Dongba scripts of the Chinese minority group Naxi in Yunnan Province, and then generates Miro’s surrealism style. Both works have been successfully applied to produce highly demanded design products. Other recent research in computational aesthetics will also be mentioned.
              </p>
            <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
              Biography:
            </p>
           
              <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
             Kang Zhang is Professor and Former Head of Computational Media and Arts, Information Hub, Hong Kong University of Science and Technology (Guangzhou), Professor of Division of Emerging Interdisciplinary Areas, HKUST, Professor Emeritus of Computer Science, The University of Texas at Dallas, and Guest Professor of China Academy of Arts. He was a Fulbright Distinguished Chair and among Stanford’s List of 2023-2025 World’s Top 2% Scientists (Lifetime Impact), and held academic positions in China, the UK, Australia and USA. Zhang's current research interests include computational aesthetics, visual languages, and generative art and design; and has published 8 books, and over 130 journal papers in these areas. He has delivered keynotes at art and design, computer science, and management conferences and forums.
            </p>
    
          </div>
        </div>
      </div>
    </section>

  <!-- third -->
  <section  class="bg-primary" id="keynotes-3">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading text-white">Keynotes (3/5)</h2>
          <hr class="light my-4">

          <p class="text-faded" style="text-align: left; color: white; font-weight: bold; font-size: 26px;">
            Keynote 3
          </p>
          <br>
              <!--    <p class="text-faded" style="text-align: left; color: white; font-weight: bold; font-size: 24px;">
            Speaker:TBD
          </p>
           -->
         <div style="float:right; clear:both; top:30px;">
            <img src="images/mangye.jpg" style="height: 200px; border-radius: 10px;" />
          </div>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Mang Ye
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
         From Guesswork to Empathy: Unlocking Emotional Insight in Multimodal Al
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
          TBD
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
          Nowadays, Multimodal Large Language Models (MLLMs) can write essays, caption photos, and chat for hours—but they still struggle with something humans do almost automatically: sensing emotions. Our work pushes MLLMs toward stronger emotion understanding by grounding them in realistic emotion-focused evaluation, teaching them to rely on the cues that actually signal feelings (rather than superficial shortcuts), and shaping their predictions to be more careful and consistent through guided reasoning and self-checking. This helps models handle a wider range of emotion-related situations in images and videos, including subtle and socially loaded content people actually encounter online, while producing answers that are more stable, better justified, and less guessy.
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
     Mang Ye is a Full Professor, Head of the Department of Intelligent Science, School of Computer Science, Wuhan University. His research interests include multimodal learning and affective computing. He has published more than 100 top-tier journal/conference papers, and has been recognized as an ESI Highly Cited Researcher by Clarivate. He serves as an Area Chair for top conferences, including CVPR, ICML, ICLR, ACM MM, and ECCV, and as an Associate Editor for IEEE TIFS and IEEE TIP. He has served as PI on more than 10 scientific research projects, including grants from the National Natural Science Foundation of China (NSFC) and the National Key R&D Program of China. His awards include the CSIG Science and Technology Award (2025), the Wu Wenjun Artificial Intelligence Science and Technology Award (2025), and the ACM China-Wuhan Rising Star Award (2023).
          </p>
             


        </div>
      </div>
    </div>
  </section> 

  <!-- fourth -->
   <section  id="keynotes-4">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading">Keynotes (4/5)</h2>
          <hr class="dark my-4">

          <p class="text-muted" style="text-align: left; font-weight: bold;font-size: 26px;">
            Keynote 4
          </p>
          <br>
                <!--  <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:TBD
          </p>
          -->
   <div style="float:right; clear:both; top:30px;">
            <img src="images/jirapun.png" style="height: 250px; border-radius: 10px;" />
          </div>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>

          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
       Jirapun Daengdej
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>

          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
         From Case-Based Reasoning to Case-Based Creativity: A Conceptual Framework for Multimodal Agents
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
           TBD
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
          While multimodal generative models offer immense expressive potential, they often function as "black boxes" lacking a mechanism to leverage structured precedents. We propose a conceptual framework for Case-Based Creativity (CBC), which utilizes a hybrid symbolic–neural architecture to bridge the gap between generative intuition and logical intent. This approach adapts foundational principles of Case-Based Reasoning (CBR) to create a memory-driven system where a symbolic layer retrieves and adapts "artistic cases"—comprising past design patterns and constraints—to guide a neural synthesis layer. By discussing modeled outputs that reflect this hybrid logic, we illustrate how agents might maintain semantic persistence and intentionality by grounding expressive neural generation within a structured symbolic context. We argue that the integration of CBR and generative AI offers a compelling conceptual direction for next-generation systems that prioritize adaptation and human–AI collaboration.
          </p>
          <p class="text-muted" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-muted" style="text-align: left; font-weight: light; font-size: 18px">
      Dr. Jirapun Daengdej is a Professor at the Graduate School of Business and Advanced Technology Management (GS-BATM), Assumption University. He is also the Founder and Chief Technology Officer of Innocop, a company specializing in designing and developing AI solutions for the financial and energy sectors. He holds a Ph.D. in Computer Science from the University of New England, Australia, where he developed one of the world’s largest Case-Based Reasoning (CBR) systems for the insurance industry. A recipient of two IBM Faculty Awards for his contributions to real-world software engineering, he previously served as an Executive Technical Advisor to global organizations. Over a period of more than 15 years, he worked on software tools used by companies such as Boeing, Rolls-Royce, and leading automotive manufacturers. His work focuses on integrating symbolic reasoning with practical AI architectures to address complex industrial challenges.
          </p>
    
        </div>
      </div>
    </div>
  </section> 

  <!-- fifth -->
  <section  class="bg-primary" id="keynotes-5">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading  text-white">Keynotes (5/5)</h2>
          <hr class="light my-4">

          <p class="text-faded" style="text-align: left; color: white; font-weight: bold; font-size: 26px;">
            Keynote 5
          </p>
          <br>
                <p class="text-faded" style="text-align: left; color: white; font-weight: bold; font-size: 24px;">
            Speaker:TBD
          </p>
        <!--    <div style="float:right; clear:both; top:30px;">
            <img src="images/Terence Broad.jpg" style="height: 200px; border-radius: 10px;" />
          </div>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Speaker:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
           Terence Broad
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Title:
          </p>

          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
            Explaining AI through artistic practice
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Time:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
           16:10-16:40, July 4, 2025
          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Abstract:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
         Generative neural networks produce media through a complex fabric of computation, contingent on large scraped datasets, where features and representations get encoded into the weights of unfathomably large data arrays, which in turn are enmeshed through complex chains of computation. The ease and realism through which this generated media is mass-produced and its almost uncanny flawlessness makes it easy to forget the complex computational contingencies that produce it. This talk will show how through the artistic practice of making targeted interventions to inputs, weights, training and inference of generative neural networks, artists are able to make critical works that reveal to us otherwise unseen aspects of these models, where the artworks themselves present new ways of understanding and making sense of these unfathomably complex computational systems. 

          </p>
          <p class="text-white" style="text-align: left; font-weight: bold; font-size: 24px;">
            Biography:
          </p>
          <p class="text-white" style="text-align: left; font-weight: light; font-size: 18px">
           Terence Broad is an artist and researcher working in London. He is a Senior Lecturer at the UAL Creative Computing Institute and has recently completed a PhD at Goldsmiths in generative AI. His art and research have been presented internationally: at conferences and journals such as SIGGRAPH, Leonardo, NeurIPS, and ICCC; and museums such as The Whitney Museum of American Art, Garage Museum of Contemporary Art, Ars Electronica, The Barbican and The Whitechapel Gallery. In 2019 He won the Grand Prize in the ICCV Computer Vision Art Gallery. His work is in the city of Geneva’s contemporary art collection.
          </p>
-->

        </div>
      </div>
    </div>
  </section> 


  <section  id="program">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading">Conference Program</h2>
          <hr class="dark my-4">
          <p style="font-size: 2em; font-weight: bold;">TBD</p>
          <!--<img src="images/aiart2025_program.jpg" style="max-width: 100%;" />-->
        </div>
      </div>
    </div>
  </section>


<section class="bg-primary" id="ps">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto text-center">
        <h2 class="section-heading  text-white">Technical Program Committee (Tentative)</h2>
        <hr class="light my-4">

        <ul class="text-faded"
            style="text-align:left; color: white; font-size:18px; line-height:1.8; margin:0; padding-left:1.2rem; list-style:disc;">
          <li>Ajay Kapur, California Institute of the Arts, USA</li>
          <li>Alan Chamberlain, University of Nottingham, Nottingham</li>
          <li>Alexander Lerch, Georgia Institute of Technology, USA</li>
          <li>Alexander Pantelyat, Johns Hopkins University, USA</li>
          <li>Bahareh Nakisa, Deakin University, Australia</li>
          <li>Baoqiang Han, China Conservatory of Music, China</li>
          <li>Baoyang Chen, Central Academy of Fine Arts, China</li>
          <li>Beici Liang, Tencent Music Entertainment Group, China</li>
          <li>Bing Li, King Abdullah University of Science and Technology, Saudi Arabia</li>
          <li>Björn W. Schuller, Imperial College London, UK</li>
          <li>Bob Sturm, KTH Royal Institute of Technology, Sweden</li>
          <li>Borou Yu, Harvard University, USA</li>
          <li>Carlos Castellanos, Rochester Institute of Technology, USA</li>
          <li>Changsheng Xu, Institute of Automation, Chinese Academy of Sciences, China</li>
          <li>Dongmei Jiang, Northwestern Polytechnical University, China</li>
          <li>Emma Young, BBC, UK</li>
          <li>Gerui Wang, Stanford University, USA</li>
          <li>Gus Xia, New York University Shanghai, China &amp; Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates</li>
          <li>Haifeng Li, Harbin Institute of Technology, China</li>
          <li>Haipeng Mi, Tsinghua University, China</li>
          <li>Hongxun Yao, Harbin Institute of Technology, China</li>
          <li>Jesse Engel, Google, USA</li>
          <li>Jia Jia, Tsinghua University, China</li>
          <li>Jiajian Min, Harvard University, USA</li>
          <li>Jianyu Fan, Microsoft, Canada</li>
          <li>Jing Wang, Beijing Institute of Technology, China</li>
          <li>John See, Multimedia University, Malaysia</li>
          <li>Juan Huang, Johns Hopkins University, USA</li>
          <li>Junping Zhang, Fudan University, China</li>
          <li>Kejun Zhang, Zhejiang University, China</li>
          <li>Ke Lv, University of Chinese Academy of Sciences, China</li>
          <li>Kenneth Fields, Central Conservatory of Music, China</li>
          <li>Lai-Kuan Wong, Multimedia University, Malaysia</li>
          <li>Lamtharn Hanoi Hantrakul, ByteDance, USA</li>
          <li>Lei Xie, Northwestern Polytechnical University, China</li>
          <li>Li Zhou, China University of Geosciences (Wuhan), China</li>
          <li>Lin Gan, Tianjin University, China</li>
          <li>Long Ye, China University of Communication, China</li>
          <li>Maosong Sun, Tsinghua University, China</li>
          <li>Mei Han, Ping An Technology Art institute, USA</li>
          <li>Mengjie Qi, China Conservatory of Music, China</li>
          <li>Ming Zhang, Nanjing Art College, China</li>
          <li>Mohammad Naim Rastgoo, Queensland University of Technology, Australia</li>
          <li>Na Qi, Beijing University of Technology, China</li>
          <li>Nick Bryan-Kinns, Queen Mary University of London, UK</li>
          <li>Nina Kraus, Northwestern University, USA</li>
          <li>Pengtao Xie, University of California, San Diego, USA</li>
          <li>Philippe Pasquier, Simon Fraser University, Canada</li>
          <li>Qin Jin, Renmin University, China</li>
          <li>Qiuqiang Kong, ByteDance, China</li>
          <li>Rebecca Fiebrink, University of London, UK</li>
          <li>Rick Taube, University of Illinois at Urbana-Champaign, USA</li>
          <li>Roger Dannenberg, Carnegie Mellon University, USA</li>
          <li>Rongfeng Li, Beijing University of Posts and Telecommunications, China</li>
          <li>Rui Wang, Institute of Information Engineering, Chinese Academy of Sciences, China</li>
          <li>Ruihua Song, Renmin University, China</li>
          <li>Shangfei Wang, University of Science and Technology of China, China</li>
          <li>Shasha Mao, Xidian University, China</li>
          <li>Shiguang Shan, Institute of Computing Technology, Chinese Academy of Sciences, China</li>
          <li>Shiqi Wang, City University of Hong Kong, China</li>
          <li>Shun Kuremoto, Uchida Yoko Co.,Ltd, Japan</li>
          <li>Si Liu, Beihang University, China</li>
          <li>Simon Lui, Huawei Technologies Co., Ltd, China</li>
          <li>Tiange Zhou, NetEase Cloud Music, China</li>
          <li>Weibei Dou, Tsinghua University, China</li>
          <li>Weiming Dong, Institute of Automation, Chinese Academy of Sciences, China</li>
          <li>Wei-Ta Chu, National Chung Cheng University, Taiwan, China</li>
          <li>Wei Li, Fudan University, China</li>
          <li>Weiwei Zhang, Dalian Maritime University, China</li>
          <li>Wei Zhong, Communication University of China, China</li>
          <li>Wen-Huang Cheng, National Chiao Tung University, Taiwan, China</li>
          <li>Wenli Zhang, Beijing University of Technology, China</li>
          <li>Xi Shao, Nanjing University of Posts and Telecommunications, China</li>
          <li>Xiaojing Liang, NetEase Cloud Music, China</li>
          <li>Xiaopeng Hong, Harbin Institute of Technology, China</li>
          <li>Xiaoyan Sun, University of Science and Technology of China, China</li>
          <li>Xiaoying Zhang, China Rehabilitation Research Center, China</li>
          <li>Xihong Wu, Peking University, China</li>
          <li>Xinfeng Zhang, University of Chinese Academy of Sciences, China</li>
          <li>Xu Tan, Microsoft Research Asia, China</li>
          <li>Yanchao Bi, Beijing Normal University, China</li>
          <li>Yi Qin, Shanghai Conservatory of Music, China</li>
          <li>Ying-Qing Xu, Tsinghua University, China</li>
          <li>Yirui Wu, Hohai University, China</li>
          <li>Yuanchun Xu, Xiaoice, China</li>
          <li>Zhiyao Duan, University of Rochester, USA</li>
        </ul>

      </div>
    </div>
  </div>
</section>


<section  id="ppl">
  <div class="container">
    <div class="row">
      <div class="col-lg-12 text-center">
        <h2 class="section-heading">Organizing Team</h2>
        <hr class="dark my-4">
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row">

      <!-- Luntian Mou -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/Luntian_Mou.jpeg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Luntian Mou
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing University of Technology</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing, China</p>
          <a href="mailto:ltmou@bjut.edu.cn" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">ltmou@bjut.edu.cn</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Luntian Mou is an Associate Professor with the School of Information Science and Technology, Beijing Institute of Artificial Intelligence (BIAI), Beijing University of Technology. He received the Ph.D. degree in computer science from the University of Chinese Academy of Sciences, China in 2012. He served as a Postdoctoral Fellow at Peking University, from 2012 to 2014. And he was a Visiting Scholar with the University of California, Irvine, from 2019 to 2020. He initiated the IEEE Workshop on Artificial Intelligence for Art Creation (AIART) in 2019, and has organized the workshop annually ever since. His current research interests include artificial intelligence, machine learning, multimedia computing, affective computing, and brain-like computing. He is the recipient of Beijing Municipal Science and Technology Advancement Award, IEEE Outstanding Contribution to Standardization Award, and AVS Outstanding Contribution on 15th Anniversary Award. He serves as a guest editor for Machine Intelligence Research, and a reviewer for many important international journals and conferences such as TIP, TAFFC, TCSVT, TITS, AAAI, etc. And he serves as a Co-Chair of System subgroup in AVS workgroup. He is a Senior Member of IEEE and CCF, and a Member of ACM, CAAI, and CSIG, and an Expert of MPEG China.
          </p>
        </div>
      </div>

      <!-- Feng Gao -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/Feng_Gao.jpeg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Feng Gao
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Peking University</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing, China</p>
          <a href="mailto:gaof@pku.edu.cn" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">gaof@pku.edu.cn</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Feng Gao is an Assistant Professor with the School of Arts, Peking University. He has long researched in the disciplinary fields of AI and art, especially in AI painting. He co-initiated the international workshop of AIART. Currently, he is also enthusiastic in virtual human. He has demonstrated his AI painting system, called Daozi, in several workshops and drawn much attention.
          </p>
        </div>
      </div>

      <!-- Kejun Zhang -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/Kejun_Zhang.jpg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Kejun Zhang
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Zhejiang University</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Hangzhou, China</p>
          <a href="mailto:zhangkejun@zju.edu.cn" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">zhangkejun@zju.edu.cn</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Kejun Zhang is a Professor with Zhejiang University, joint PhD supervisor on Design and Computer Science, Dean of Department of Industrial Design at College of Computer Science of Zhejiang University. He received his PhD degree from College of Computer Science and Technology, Zhejiang University in 2010. From 2008 to 2009, He was a visiting research scholar of University of Illinois at Urbana-Champaign, USA. In June 2013, he became a faculty of the College of Computer Science and Technology at Zhejiang University. His current research interests include Affective Computing, Design Science, Artificial Intelligence, Multimedia Computing and the understanding, modelling and innovation design of products and social management by computational means. He is now the PI of National Science Foundation of China, Co-PI of National Key Research and Development Program of China, and PIs of ten more other research programs. He has authored 4 books, more than 40 scientific papers.
          </p>
        </div>
      </div>

      <!-- Haonan Cheng -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/haonancheng.jpg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Haonan Cheng
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Communication University of China</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Beijing, China</p>
          <!-- 这里原来少了 mailto:，我顺便修正 -->
          <a href="mailto:haonancheng@cuc.edu.cn" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">haonancheng@cuc.edu.cn</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Haonan Cheng is a Professor with the State Key Laboratory of Media Convergence and Communication, Communication University of China, mainly focuses on audio information processing, audio-visual cross modal generation and forgery detection. She became the first technical expert in China to be awarded the Asia-Pacific Young Engineer Prize by ABU in 2024, and was selected for the Beijing National Governance and Young Talent Cultivation Program in 2025. In recent years, she has published more than 50 SCI/EI papers in IEEE TOG, TIFS, TASLP, SIGGRAPH, IEEE VR, IJCAI, AAAI, ACM MM, etc. She has been authorized 2 national invention patents, and won the Excellent Paper Award in the 5th CSIG China Media Forensics and Security Conference, and Best Poster Paper Award in the 20th International Forum on Digital Multimedia Communications. She was funded by more than 10 projects, including National Natural Science Foundation of China, National Key R&amp;D Program, National Social Science Foundation of China, and Medium and Long-term Science and Technology Program for Radio, Television and Audiovisual Network, etc. She serves as a member of the Multimedia Specialized Committee of the Chinese Society of Image and Graphics, the Program Chair of the International Forum on Digital Multimedia Communications, the Forum Chair of the China Multimedia Conference, and the Session Chair of ACM MM and other international conferences.
          </p>
        </div>
      </div>

      <!-- Ambarish Natu -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/Ambarish_Natu.jpg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Ambarish Natu
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Australian Government</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Australian Capital Territory, Australia</p>
          <a href="mailto:ambarish.natu@gmail.com" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">ambarish.natu@gmail.com</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Ambarish Natu is with the Australian Government. After graduating from University of New South Wales, Sydney, Ambarish has held positions as a visiting researcher in Italy and Taiwan, worked for industry in United Kingdom and the United States of America and for the past ten years has been working in the Australian Government. For the past 17 years, Ambarish has led the development of five international standards under the auspices of the International Standards Organization (ISO) popularly known as JPEG (Joint Photographic Experts Group). He is the recipient of the ISO/IEC certificate for contributions to technology standards. Ambarish is highly active in the area of international standardization and voicing Australian concerns in the area of JPEG and MPEG (Motion Pictures Experts Group) standardization. He previously initiated an effort in the area of standardization relating to Privacy and Security in the Multimedia Context both within JPEG and MPEG standard bodies. In 2015, Ambarish was the recipient of the prestigious Neville Thiele Award and the Canberra Professional Engineer of the Year by Engineers Australia. Ambarish currently works as an ICT Specialist for the Australian Government. Ambarish is a Fellow of the Australian Computer Society and Engineers Australia. Ambarish also serves on the IVMSP TC and the Autonomous Systems Initiative of the IEEE Signal Processing Society. Ambarish has also been General Chair of DICTA 2018, ICME 2023 and TENSYMP 2023 in the past. Ambarish has keen interest in next generation data and analytics technologies that will change the course of the way we interact with in the world.
          </p>
        </div>
      </div>

      <!-- Gerui Wang -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/geruiwang.jpg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Gerui Wang
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Stanford University</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">California, USA</p>
          <!-- 这里原来 mailto 写错了，我顺便修正 -->
          <a href="mailto:grwang@stanford.edu" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">grwang@stanford.edu</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Gerui Wang is a Lecturer at Stanford University Center for East Asian Studies, where she teaches classes on contemporary art, AI and posthumanism. Her research interests span arts, public policy, environment, and emerging technologies. She is a member of the Alan Turing Institute AI&amp;Arts Research Group. With her background in art history, she has published in the Journal of Chinese History and Newsletter for International China Studies. Gerui's book Sustaining Landscapes: Governance and Ecology in Chinese Visual Culture is forthcoming in 2025. Her research briefs on AI, robotics, media, and society are frequently featured in public venues including Forbes, Alan Turing Institute's AI and Art Forum, Asia Times, and South China Morning Post. Gerui holds a doctorate in art history from the University of Michigan.
          </p>
        </div>
      </div>

      <!-- Ling Fan -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/Ling_Fan.jpg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Ling Fan
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">Tezign.com</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Tongji University Design Artificial Intelligence Lab</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">Shanghai, China</p>
          <a href="mailto:lfan@tongji.edu.cn" class="text-white" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">lfan@tongji.edu.cn</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Ling Fan is a Scholar and Entrepreneur to bridge machine intelligence with creativity. He is the founding chair and professor of Tongji University Design Artificial Intelligence Lab. Before, he held teaching position at the University of California at Berkeley and China Central Academy of Fine Arts. Dr. Fan co-founded Tezign.com, a leading technology start-up with the mission to build digital infrastructure for creative contents. Tezign is backed by top VCs like Sequoia Capital and Hearst Ventures. Dr. Fan is a World Economic Forum Young Global Leader, an Aspen Institute China Fellow, and Youth Committee member at the Future Forum. He is also a member of IEEE Global Council for Extended Intelligence. Dr. Fan received his doctoral degree from Harvard University and master's degree from Princeton University. He recently published From Universality of Computation to the Universality of Imagination, a book on how machine intelligence would influence human creativity.
          </p>
        </div>
      </div>

      <!-- Terence Broad -->
      <div class="col-lg-6 col-md-8 text-center">
        <div class="service-box mt-5 mx-auto">
          <img class="mb-3" src="images/Terence Broad.jpg" style="height: 200px; border-radius: 20px;" />

          <h5 class="mb-3">
            <a target="_blank" rel="noopener noreferrer" class="text-muted" style="text-decoration: none;">
              Terence Broad
            </a>
          </h5>

          <p class="text-muted mb-0" style="font-size: 0.9em;">University of the Arts London</p>
          <p class="text-muted mb-0" style="font-size: 0.9em;">London, The United Kingdom</p>
          <a href="mailto:t.broad@arts.ac.uk" class="text-muted" style="text-decoration:none;">
            <p class="mb-0" style="font-size: 0.9em;">t.broad@arts.ac.uk</p>
          </a>
          <br>

          <p class="text-muted mb-0" style="font-size: 0.9em;" align="left">
            Dr. Terence Broad is an Artist and Researcher working in London. He is a Senior Lecturer at the UAL Creative Computing Institute and has recently completed a PhD at Goldsmiths in generative AI. His art and research have been presented internationally: at conferences and journals such as SIGGRAPH, Leonardo, NeurIPS, and ICCC; and museums such as The Whitney Museum of American Art, Garage Museum of Contemporary Art, Ars Electronica, The Barbican and The Whitechapel Gallery. In 2019 He won the Grand Prize in the ICCV Computer Vision Art Gallery. His work is in the city of Geneva’s contemporary art collection.
          </p>
        </div>
      </div>

    </div>
  </div>
</section>




 <section class="bg-primary" id="MIR">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto text-center">

        <h2 class="section-heading text-white">Partners</h2>
        <hr class="light my-4">

        <!-- Partner title -->
        <h5 class="section-heading text-white" style="text-align:left; line-height:1.8">
          <strong>Partner1: Machine Intelligence Research</strong>
        </h5>

        <!-- Description -->
        <div class="text-faded" style="text-align:left; color:white; font-size:18px; line-height:1.8">
         Machine Intelligence Research (IF:8.7, JCR Q1), published by Springer, and sponsored by Institute of Automation, Chinese Academy of Sciences, is formally released in 2022. The journal publishes high-quality papers on original theoretical and experimental research in artificial intelligence, targets special issues on emerging topics and specific subjects, and strives to bridge the gap between theoretical research and practical applications. The journal has been indexed by ESCI, EI, Scopus, CSCD, etc.
        </div>

        <!-- Topics -->
        <div class="text-faded" style="text-align:left; color:white; font-size:18px; line-height:1.8; margin-top:15px">
         Topics of Machine Intelligence Research include: AI Fundamentals, Brain-Inspired Intelligence, Pattern Recognition & Machine Learning, Machine Vision, Speech and Language Processing, Embodied Intelligence and Robotics, Knowledge Management & Data Mining, and Applications of Machine Intelligence.
        </div>

        <!-- List -->
        <ul style="color:white; text-align:left;color:white; margin-top:20px">

          <li>
            <div class="text-faded" style="font-size:18px; color:white;line-height:1.8">
              <strong>MIR official websites</strong>
              <ul>
                <li>
                  <a href="https://www.springer.com/journal/11633" target="_blank">
                    https://www.springer.com/journal/11633
                  </a>
                </li>
                <li>
                  <a href="https://www.mi-research.net" target="_blank">
                    https://www.mi-research.net
                  </a>
                </li>
              </ul>
            </div>
          </li>

          <li>
            <div class="text-faded" style="font-size:18px;color:white; line-height:1.8">
             <strong>MIR Editor-in-Chief </strong>
              <ul>
                <li>Tan Tieniu, Institute of Automation, Chinese Academy of Sciences</li>
              </ul>
            </div>
          </li>

          <li>
            <div class="text-faded" style="font-size:18px;color:white; line-height:1.8">
               <strong>MIR Associate Editors-in-Chief</strong>
              <ul>
                <li>Yike Guo, Hong Kong University of Science and Technology, China</li>
                <li>Brian C. Lovell, The University of Queensland, Australia</li>
                <li>Danilo P. Mandic, Imperial College London, UK</li>
                <li>Liang Wang, Chinese Academy of Sciences, China</li>
              </ul>
            </div>
          </li>

        </ul>

        <!-- Image -->
        <div style="text-align:center; margin-top:30px">
          <img src="images/MIR1.png" alt="Machine Intelligence Research" 
               style="max-width:100%; height:auto;">
        </div>
         <br> <br>
          <!-- Partner title -->
        <h5 class="section-heading text-white" style="text-align:left; line-height:1.8">
          <strong>Partner2: The Transactions on Artificial Intelligence </strong>
        </h5>

        <!-- Description -->
        <div class="text-faded" style="text-align:left; color:white; font-size:18px; line-height:1.8">
         The Transactions on Artificial Intelligence (TAI) is a peer-reviewed, open-access journal dedicated to advancing trustworthy, explainable, and human-centered AI. The journal highlights emerging frontiers—including generative AI, autonomous systems, AI safety, and data-centric intelligence—while maintaining strong coverage of core AI theory and methodologies.
        </div>

        <!-- List -->
        <ul style="color:white; text-align:left; margin-top:20px">

          <li>
            <div class="text-faded" style="font-size:18px; color:white;line-height:1.8">
              <strong>TAI official website</strong>
              <ul>
                <li>
                  <a href="https://www.sciltp.com/journals/tai" target="_blank">
                    https://www.sciltp.com/journals/tai
                  </a>
                </li>
              </ul>
            </div>
          </li>
          <li>
            <div class="text-faded" style="font-size:18px;color:white; line-height:1.8">
             <strong>TAI Editor-in-Chief </strong>
              <ul>
                <li>Prof. Dapeng Oliver Wu, City University of Hong Kong, Hong Kong</li>
              </ul>
            </div>
          </li>

       </ul>
        <!-- Image -->
   <div style="text-align:center; margin-top:30px">
  <img class="mb-3"
       src="images/TAI.jpg"
       alt="TAI"
       style="height:auto; width:100%; max-width:400px; border-radius:20px;" />
  <br>
</div>
        <br><br>  
<h5 class="section-heading text-white" style="text-align:left; line-height:1.8">
           <strong>Partner3: The China Society of Image and Graphics</strong>
         </h5>
         <div class="text-faded" style="text-align:left; color:white; font-size:18px; line-height:1.8">
           The China Society of Image and Graphics (CSIG), founded in 1990, is a national first-tier academic society approved by the Ministry of Civil Affairs of China and an official member of the China Association for Science and Technology. 
         </div>
          <div class="text-faded" style="text-align:left; color:white; font-size:18px; line-height:1.8; margin-top:15px">
          CSIG's professional fields include digital image processing, image understanding, computer vision, image compression and transmission, stereoscopic technology, scientific computing visualization, virtual reality, multimedia technology, pattern recognition, computer graphics, medical image processing, computer animation, spatial information systems, and more.
         </div>
         <div class="text-faded" style="text-align:left; color:white; font-size:18px; line-height:1.8; margin-top:15px">
           In recent years, CSIG aims to advance the development of related fields and serve national strategic needs. It actively engages in academic exchanges, technical training, exhibitions, science popularization, policy consultation, evaluation of scientific and technological achievements, transformation of research outcomes, talent recommendation, and international cooperation. By serving as a bridge connecting scientists and engineers, CSIG has become a vital social force in promoting innovation and development in the field of image and graphics.
         </div>
         <ul style="color:white; text-align:left; margin-top:20px">

          <li>
            <div class="text-faded" style="font-size:18px; color:white;line-height:1.8">
              <strong>CSIG official website</strong>
              <ul>
                <li>
                  <a href="http://www.csig.org.cn/" target="_blank">
                    http://www.csig.org.cn/
                  </a>
                </li>
              </ul>
            </div>
          </li>

       </ul>
       <div style="text-align:center; margin-top:30px">
          <img src="images/csig.png" alt="CSIG"
       style="width:100%; max-width:320px; height:auto; border-radius:16px;" />
       </div>

      </div>
    </div>
  </div>
</section>


  <section  id="sponsorship">
    <div class="container">
      <div class="row">
        <div class="col-lg-10 mx-auto text-center">
          <h2 class="section-heading">Sponsorship</h2>
          <hr class="dark my-4">

            <h5 class="section-heading" style="text-align: left; line-height: 1.8">
            <p style="font-size: 1.5em; text-align: center; margin-top: 20px;">
          TBD
        </p>
         

       <!--      <div class="text-muted mb-0" style="text-align: left;color: black;font-size: 18px;line-height: 1.8">

              <strong>Platinum Level (RMB￥100,000)</strong><br>
              - Large booths (size TBD)<br>
              - Invitation to give an industry keynote speech<br>
              - Logo on AIART 2024 official website with description and link to sponsor website<br>
              - Logo on workshop handbook and presentation material (under Platinum Level)<br>
              - One on one negotiation for special requirements.<br><br>

              <strong>Gold Level (RMB￥50,000)</strong><br>
              - Medium booths (size TBD)<br>
              - Participation in related industry panel<br>
              - Logo on AIART 2024 official website with short description and link to sponsor website<br>
              - Logo on workshop handbook and presentation material (under Gold Level)<br>
              - One on one negotiation for special requirements.<br><br>

              <strong>Silver Level (RMB￥20,000)</strong><br>
              - Small booths (size TBD)<br>
              - Logo on AIART 2024 official website with link to sponsor website<br>
              - Logo on workshop handbook and presentation material (under Silver Level)<br><br>

            </div> -->

         <!--       </h5>
          <br><br>
        <a>
  <img src="images/douyin_logo1.png" style="width: 100%; max-width: 400px; height: auto; display: block; margin: 0 auto;" />
</a>

          <br><br><br>


          <h5 class="section-heading text-white" style="text-align: left; line-height: 1.8">

               <div class="text-muted mb-4" style="text-align: justify; color: black; font-size: 18px; line-height: 1.8; font-weight: normal;">
              Douyin (抖音), is a platform that helps users express themselves and record their good life. Douyin is leading the short-video app universe in China with more than 755 million monthly active users (MAU) and continues to grow at a high rate. Douyin covers a wide range of video content, including technology, art, aesthetics, daily life, etc., and is committed to enriching the life of every user. <br><br>

              <!-- Tezign has raised the D1 round of financing and become the only Contech unicorn enterprise with a
              valuation of more than US $1 billion. Tezign is backed by world recognized investors including Temasek,
              Sequoia Capital, Hearst Ventures, among others. Tezign has partnered with more than 200 medium to large
              enterprises such as Alibaba, Unilever, ByteDance, PepsiCo, Shiseido, P&G, Starbucks, McDonald's, Heinz,
              Mars, Budweiser, Adidas, Xtep, Ubras, Lenovo, Midea, Tencent, L'Oreal, Danone, Porsche, Audi, Volvo,
              Aptar, Bosch, Stanley Black & Decker etc to upgrade their content strategies. It has produced more than
              150,000 creative content assets and formed a one hundred million level content asset management scale.
              Tezign's content ecosystem has gathered more than 50,000 content creators. <br><br>

              Tezign was a National Industrial Design Center issued by the Ministry of industry and information
              technology, awarded as TOP.1 in China in the list of The Information 50 startups, Fast Company top 50 of
              China's best innovative companies, Forbes High Growth Gazelle, Forbes China Top 10 intelligent design
              enterprises, Hurun Group List of Global Unicorn, and was named by Forrester, a leading independent global
              technology and market research company, in the Research Report "Now Tech: Marketing Resource Management,
              Q12022" (MRM) as the only MRM vendor in the contech segment Asia Pacific. <br><br>

              On 6th-7th May 2023, Tezign will launch the AIGC Builder and Creator Conference in Shanghai, to create
              maximum two-way interaction among AIGC creators and builders in various quality forms. Stay tuned by
              following the official Wechat account: Who is AIGC. <br> -->
            </div>

          </h5>
        </div>
      </div>
    </div>
  </section>

  
 <section class="bg-primary" id="Gallery">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto text-center">
        <h2 class="section-heading text-white">Gallery</h2>
        <hr class="light my-4">
             <h5 class="section-heading  text-white" style="text-align: left; line-height: 1.8">
            <p style="font-size: 1.5em; text-align: center; margin-top: 20px;">
          TBD
        </p>
         
      <!-- 标题
        <div class="text-faded" style="text-align: left; color: white; font-size: 24px; line-height: 1.8; font-weight: normal;">
          <strong>ICME AIART GALLERY</strong>
        </div>

        <!-- 描述 
        <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
          As a highlight of IEEE AIART 2025, ICME AIART Gallery brings together AI-powered artworks from around the world. It’s a global platform for creativity and communication, exploring how AI is shaping the landscape and future of art.
        </div>
        <br>
        <!-- 图片居中 
        <div class="text-center">
          <img src="images/gallery.png" alt="AIART Gallery Image" style="max-width: 80%; height: auto;" />
        </div>
        <br>
        <!-- 副标题 
     <div class="text-faded" style="text-align: center; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
  <strong>Intelligence Reimagined: The Convergence of Art and AI</strong>
</div>

        <!-- 外部链接
     <div class="text-faded" style="text-align: center; color: white; font-size: 16px; line-height: 1.8; font-weight: normal;">
          <a href="https://sites.google.com/view/2025aiart-gallery" target="_blank">
            https://sites.google.com/view/2025aiart-gallery
          </a>
        </div>

        <!-- 时间地点 
 <div class="text-faded" style="text-align: center; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
  July 1–July 4, 2025
</div>
 <div class="text-faded" style="text-align: center; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
  at La Cité Nantes Congress Centre, France
</div>

        <!-- 投稿说明
 <div class="text-faded" style="text-align: center; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
  <strong>Call for AI artworks</strong>
</div>

        <!-- 投稿链接
     <div class="text-faded" style="text-align: center; color: white; font-size: 16px; font-style: italic; line-height: 1.8; font-weight: normal;">
          (Submission Link: 
          <a href="https://forms.office.com/r/HjQ8MYY18q?origin=lprLink" target="_blank">
            https://forms.office.com/r/HjQ8MYY18q?origin=lprLink
          </a>)
   </div>
        <!-- 补充说明 
        <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
          <strong>Both AI-generated or AI-assisted artworks and traditional art exploring AI-related themes are welcome.</strong>
        </div>
        <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
          <strong>Submission Deadline:</strong> Now until April 30, 2025.
        </div>
        <br><br>
       <div class="text-faded" style="text-align: left; color: white; font-size: 24px; line-height: 1.8; font-weight: normal;">
          <strong>Douyin Digital Art Calling Order - AIART Special Exhibition</strong>
        </div>     
     <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
        In order to promote the development of AI art and to present the diversity of AI art in China and around the world, Douyin ART and IEEE AIART Workshop have launched the “Douyin Digital Art Calling Order - AIART Special Exhibition in Nantes, France”.
        </div>
  <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
      From today to June 26, 2025, post your AI artwork on the Douyin platform with the tag #Douyin Digital Art Call #AIART Exhibition in Nantes, France, and you can apply for the program. The IEEE AIART Technical Program Committee will select the best works online. Selected artists will have the opportunity to send their works to the AIART Special Exhibition in Nantes, France.
        </div>
        <br>
        <!-- 图片居中 
        <div class="text-center">
          <img src="images/douyin.jpg" alt="AIART Gallery Image" style="max-width: 80%; height: auto;" />
        </div>
<br>
      <!-- 副标题 
     <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
  <strong>Event Video Link:</strong>
</div>
        <!-- 外部链接 
     <div class="text-faded" style="text-align: left; color: white; font-size: 18px; line-height: 1.8; font-weight: normal;">
          <a href="https://www.douyin.com/note/7517866188744658226" target="_blank">
            https://www.douyin.com/note/7517866188744658226
          </a>
        </div>
        <br><br>-->
      </div>
    </div>
  </div>
</section>


 
<section id="Book">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto text-center">

        <h2 class="section-heading">Book</h2>
        <hr class="dark my-4">

        <div class="text-center mt-4">

          <!-- Title -->
          <div class="mb-3"
               style="text-align:left; color: black; font-size:24px; line-height:1.8;">
            <strong>Artificial Intelligence for Art Creation and Understanding</strong>
          </div>

          <!-- Editor -->
          <div class="mb-4 text-muted"
               style="text-align:left; font-size:20px; line-height:1.8; font-style:italic; font-weight:normal;">
            Edited By
            <a href="https://www.taylorfrancis.com/search?contributorName=Luntian%20Mou&contributorRole=editor&redirectFromPDP=true&context=ubx"
               target="_blank">
              Luntian Mou
            </a>
          </div>

      
          <p class="text-muted mb-4"
             style="text-align:justify; font-size:18px; line-height:1.8; font-weight:normal;color: black;">
            <strong>eBook Published:</strong> 29 August 2024<br>
            <strong>Imprint:</strong> CRC Press<br>
            <strong>DOI:</strong>
            <a href="https://doi.org/10.1201/9781003406273" target="_blank">
              https://doi.org/10.1201/9781003406273
            </a>
          </p>

          <img class="mb-3" src="images/book.jpg"
               style="height:auto; width:100%; max-width:400px; border-radius:20px;" />

          <br>

          <p class="mb-3" style="text-align:left; color: black; font-size:24px;">
            <strong>ABSTRACT</strong>
          </p>

          <p class="text-muted mb-0"
             style="text-align:justify; font-size:18px; line-height:1.8; font-weight:normal;">
            AI-Generated Content (AIGC) is a revolutionary engine for digital content generation. In the area of art, AI has achieved remarkable advancements. AI is capable of not only creating paintings or music comparable to human masterpieces, but it also understands and appreciates artwork. For professionals and amateurs, AI is an enabling tool and an opportunity to enjoy a new world of art.
          </p>

          <p class="text-muted mb-0"
             style="text-align:justify; font-size:18px; line-height:1.8; font-weight:normal;">
            This book aims to present the state-of-the-art AI technologies for art creation, understanding, and evaluation. The contents include a survey on cross-modal generation of visual and auditory content, explainable AI and music, AI-enabled robotic theater for Chinese folk art, AI for ancient Chinese music restoration and reproduction, AI for brainwave opera, artistic text style transfer, data-driven automatic choreography, Human-AI collaborative sketching, personalized music recommendation and generation based on emotion and memory (MemoMusic), understanding music and emotion from the brain, music question answering, emotional quality evaluation for generated music, and AI for image aesthetic evaluation.
          </p>

          <p class="text-muted mb-0"
             style="text-align:justify; font-size:18px; line-height:1.8; font-weight:normal;">
            The key features of the book are as follows:
          </p>

          <ul style="text-align:left; font-size:18px; line-height:1.8; color: black;">
            <li class="text-muted"><strong>AI for Art is a fascinating cross-disciplinary field for the academic community as well as the public.</strong></li>
            <li class="text-muted"><strong>Each chapter is an independent interesting topic, which provides an entry for corresponding readers.</strong></li>
            <li class="text-muted"><strong>It presents SOTA AI technologies for art creation and understanding.</strong></li>
            <li class="text-muted"><strong>The artistry and appreciation of the book is wide-ranging – for example, the combination of AI with traditional Chinese art.</strong></li>
          </ul>

          <br>

          <p class="text-muted mb-4"
             style="text-align:justify; font-size:18px; line-height:1.8; font-weight:normal;">
            This book is dedicated to the international cross-disciplinary AI Art community: professors, students, researchers, and engineers from AI (machine learning, computer vision, multimedia computing, affective computing, robotics, etc.), art (painting, music, dance, fashion, design, etc.), cognitive science, and psychology. General audiences can also benefit from this book.
          </p>

          <p class="mb-3" style="text-align:left; color: black; font-size:24px;">
            <strong>Purchase Links:</strong>
          </p>

          <ul style="text-align:left; font-size:18px; line-height:1.8;">
            <li class="text-muted">
              <a href="https://www.amazon.com/Artificial-Intelligence-Understanding-Multimedia-Communication/dp/1032523603" target="_blank">
                https://www.amazon.com/Artificial-Intelligence-Understanding-Multimedia-Communication/dp/1032523603
              </a>
            </li>
            <li class="text-muted">
              <a href="https://www.routledge.com/Artificial-Intelligence-for-Art-Creation-and-Understanding/Mou/p/book/9781032523606" target="_blank">
                https://www.routledge.com/Artificial-Intelligence-for-Art-Creation-and-Understanding/Mou/p/book/9781032523606
              </a>
            </li>
            <li class="text-muted">
              <a href="https://item.jd.com/10112811679589.html" target="_blank">
                https://item.jd.com/10112811679589.html
              </a>
            </li>
            <li class="text-muted">
              <a href="https://product.dangdang.com/11804604409.html" target="_blank">
                https://product.dangdang.com/11804604409.html
              </a>
            </li>
          </ul>

        </div>
      </div>
    </div>
  </div>
</section>





<section class="bg-primary" id="history">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto text-center">

        <h2 class="section-heading text-white">History</h2>
        <hr class="light my-4">

        <div class="text-white"
             style="text-align:left; font-size:18px color: white; line-height:1.8;">

          <ul style="padding-left:1.2rem; margin-bottom:1rem;">
            <li>
              AIART 2025:
              <a href="https://aiart-2025.github.io/" target="_blank">
                https://aiart-2025.github.io/
              </a>, Nantes, France, with ICME 2025
            </li>
            <li>
              AIART 2024:
              <a href="https://aiart2024.github.io/" target="_blank">
                https://aiart2024.github.io/
              </a>, Niagra Falls, Canada, with ICME 2024
            </li>
            <li>
              AIART 2023:
              <a href="https://aiart2023.github.io/" target="_blank">
                https://aiart2023.github.io/
              </a>, Brisbane, Australia, with ICME 2023
            </li>
            <li>
              AIART 2022:
              <a href="https://aiart2022.github.io/" target="_blank">
                https://aiart2022.github.io/
              </a>, online, with ICME 2022
            </li>
            <li>
              AIART 2021:
              <a href="https://aiart2021.github.io/" target="_blank">
                https://aiart2021.github.io/
              </a>, online, with MIPR 2021
            </li>
            <li>
              AIART 2020:
              <a href="https://aiart2020.github.io/" target="_blank">
                https://aiart2020.github.io/
              </a>, online, with MIPR 2020
            </li>
            <li>
              AIART 2019:
              <a href="https://aiart2019.github.io/" target="_blank">
                https://aiart2019.github.io/
              </a>, San Jose, USA, with MIPR 2019
            </li>
          </ul>
        </div>

      </div>
    </div>
  </div>
</section>





  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
  <script src="vendor/scrollreveal/scrollreveal.min.js"></script>
  <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/creative.min.js"></script>

</body>

</html>
